# Knowledge distillation experiments

Algorithms: KD, FitNet(Hint), PKT

Files:
1. Train and evaluate models before distillation ([exp0_baseline_models.py](exp_cifar/exp0_baseline_models.py))
2. Knowledge distillation ([exp1_retrieval_transfer.py](exp_cifar/exp1_retrieval_transfer.py))
3. Print the evaluation results ([exp2_print_results.py](exp_cifar/exp2_print_results.py))

Download pretrained ResNet-18 model [Link](https://www.dropbox.com/s/6jvhn3pjedqika1/resnet18_cifar10.model?dl=0)


